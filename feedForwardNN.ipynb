{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b,30.83,0,u,g,w,v,1.25,t,t,01,f,g,00202,0,+\n",
      "a,58.67,4.46,u,g,q,h,3.04,t,t,06,f,g,00043,560,+\n",
      "a,24.50,0.5,u,g,q,h,1.5,t,f,0,f,g,00280,824,+\n",
      "b,27.83,1.54,u,g,w,v,3.75,t,t,05,t,g,00100,3,+\n",
      "b,20.17,5.625,u,g,w,v,1.71,t,f,0,f,s,00120,0,+\n",
      "b,32.08,4,u,g,m,v,2.5,t,f,0,t,g,00360,0,+\n",
      "b,33.17,1.04,u,g,r,h,6.5,t,f,0,t,g,00164,31285,+\n",
      "a,22.92,11.585,u,g,cc,v,0.04,t,f,0,f,g,00080,1349,+\n",
      "b,54.42,0.5,y,p,k,h,3.96,t,f,0,f,g,00180,314,+\n",
      "b,42.50,4.915,y,p,w,v,3.165,t,f,0,t,g,00052,1442,+\n",
      "b,22.08,0.83,u,g,c,h,2.165,f,f,0,t,g,00128,0,+\n",
      "b,29.92,1.835,u,g,c,h,4.335,t,f,0,f,g,00260,200,+\n",
      "a,38.25,6,u,g,k,v,1,t,f,0,t,g,00000,0,+\n",
      "b,48.08,6.04,u,g,k,v,0.04,f,f,0,f,g,00000,2690,+\n",
      "a,45.83,10.5,u,g,q,v,5,t,t,07,t,g,00000,0,+\n",
      "b,36.67,4.415,y,p,k,v,0.25,t,t,10,t,g,00320,0,+\n",
      "b,28.25,0.875,u,g,m,v,0.96,t,t,03,t,g,00396,0,+\n",
      "a,23.25,5.875,u,g,q,v,3.17,t,t,10,f,g,00120,245,+\n",
      "b,21.83,0.25,u,g,d,h,0.665,t,f,0,t,g,00000,0,+\n",
      "a,19.17,8.585,u,g,cc,h,0.75,t,t,07,f,g,00096,0,+\n",
      "b,25.00,11.25,u,g,c,v,2.5,t,t,17,f,g,00200,1208,+\n",
      "b,23.25,1,u,g,c,v,0.835,t,f,0,f,s,00300,0,+\n",
      "a,47.75,8,u,g,c,v,7.875,t,t,06,t,g,00000,1260,+\n",
      "a,27.42,14.5,u,g,x,h,3.085,t,t,01,f,g,00120,11,+\n",
      "a,41.17,6.5,u,g,q,v,0.5,t,t,03,t,g,00145,0,+\n",
      "a,15.83,0.585,u,g,c,h,1.5,t,t,02,f,g,00100,0,+\n",
      "a,47.00,13,u,g,i,bb,5.165,t,t,09,t,g,00000,0,+\n",
      "b,56.58,18.5,u,g,d,bb,15,t,t,17,t,g,00000,0,+\n",
      "b,57.42,8.5,u,g,e,h,7,t,t,03,f,g,00000,0,+\n",
      "b,42.08,1.04,u,g,w,v,5,t,t,06,t,g,00500,10000,+\n",
      "b,29.25,14.79,u,g,aa,v,5.04,t,t,05,t,g,00168,0,+\n",
      "b,42.00,9.79,u,g,x,h,7.96,t,t,08,f,g,00000,0,+\n",
      "b,49.50,7.585,u,g,i,bb,7.585,t,t,15,t,g,00000,5000,+\n",
      "a,36.75,5.125,u,g,e,v,5,t,f,0,t,g,00000,4000,+\n",
      "a,22.58,10.75,u,g,q,v,0.415,t,t,05,t,g,00000,560,+\n",
      "b,27.83,1.5,u,g,w,v,2,t,t,11,t,g,00434,35,+\n",
      "b,27.25,1.585,u,g,cc,h,1.835,t,t,12,t,g,00583,713,+\n",
      "a,23.00,11.75,u,g,x,h,0.5,t,t,02,t,g,00300,551,+\n",
      "b,27.75,0.585,y,p,cc,v,0.25,t,t,02,f,g,00260,500,+\n",
      "b,54.58,9.415,u,g,ff,ff,14.415,t,t,11,t,g,00030,300,+\n",
      "b,34.17,9.17,u,g,c,v,4.5,t,t,12,t,g,00000,221,+\n",
      "b,28.92,15,u,g,c,h,5.335,t,t,11,f,g,00000,2283,+\n",
      "b,29.67,1.415,u,g,w,h,0.75,t,t,01,f,g,00240,100,+\n",
      "b,39.58,13.915,u,g,w,v,8.625,t,t,06,t,g,00070,0,+\n",
      "b,56.42,28,y,p,c,v,28.5,t,t,40,f,g,00000,15,+\n",
      "b,54.33,6.75,u,g,c,h,2.625,t,t,11,t,g,00000,284,+\n",
      "a,41.00,2.04,y,p,q,h,0.125,t,t,23,t,g,00455,1236,+\n",
      "b,31.92,4.46,u,g,cc,h,6.04,t,t,03,f,g,00311,300,+\n",
      "b,41.50,1.54,u,g,i,bb,3.5,f,f,0,f,g,00216,0,+\n",
      "b,23.92,0.665,u,g,c,v,0.165,f,f,0,f,g,00100,0,+\n",
      "a,25.75,0.5,u,g,c,h,0.875,t,f,0,t,g,00491,0,+\n",
      "b,26.00,1,u,g,q,v,1.75,t,f,0,t,g,00280,0,+\n",
      "b,37.42,2.04,u,g,w,v,0.04,t,f,0,t,g,00400,5800,+\n",
      "b,34.92,2.5,u,g,w,v,0,t,f,0,t,g,00239,200,+\n",
      "b,34.25,3,u,g,cc,h,7.415,t,f,0,t,g,00000,0,+\n",
      "b,23.33,11.625,y,p,w,v,0.835,t,f,0,t,g,00160,300,+\n",
      "b,23.17,0,u,g,cc,v,0.085,t,f,0,f,g,00000,0,+\n",
      "b,44.33,0.5,u,g,i,h,5,t,f,0,t,g,00320,0,+\n",
      "b,35.17,4.5,u,g,x,h,5.75,f,f,0,t,s,00711,0,+\n",
      "b,43.25,3,u,g,q,h,6,t,t,11,f,g,00080,0,+\n",
      "b,56.75,12.25,u,g,m,v,1.25,t,t,04,t,g,00200,0,+\n",
      "b,31.67,16.165,u,g,d,v,3,t,t,09,f,g,00250,730,+\n",
      "a,23.42,0.79,y,p,q,v,1.5,t,t,02,t,g,00080,400,+\n",
      "a,20.42,0.835,u,g,q,v,1.585,t,t,01,f,g,00000,0,+\n",
      "b,26.67,4.25,u,g,cc,v,4.29,t,t,01,t,g,00120,0,+\n",
      "b,34.17,1.54,u,g,cc,v,1.54,t,t,01,t,g,00520,50000,+\n",
      "a,36.00,1,u,g,c,v,2,t,t,11,f,g,00000,456,+\n",
      "b,25.50,0.375,u,g,m,v,0.25,t,t,03,f,g,00260,15108,+\n",
      "b,19.42,6.5,u,g,w,h,1.46,t,t,07,f,g,00080,2954,+\n",
      "b,35.17,25.125,u,g,x,h,1.625,t,t,01,t,g,00515,500,+\n",
      "b,32.33,7.5,u,g,e,bb,1.585,t,f,0,t,s,00420,0,-\n",
      "a,38.58,5,u,g,cc,v,13.5,t,f,0,t,g,00980,0,-\n",
      "b,44.25,0.5,u,g,m,v,10.75,t,f,0,f,s,00400,0,-\n",
      "b,44.83,7,y,p,c,v,1.625,f,f,0,f,g,00160,2,-\n",
      "b,20.67,5.29,u,g,q,v,0.375,t,t,01,f,g,00160,0,-\n",
      "b,34.08,6.5,u,g,aa,v,0.125,t,f,0,t,g,00443,0,-\n",
      "a,19.17,0.585,y,p,aa,v,0.585,t,f,0,t,g,00160,0,-\n",
      "b,21.67,1.165,y,p,k,v,2.5,t,t,01,f,g,00180,20,-\n",
      "b,21.50,9.75,u,g,c,v,0.25,t,f,0,f,g,00140,0,-\n",
      "b,49.58,19,u,g,ff,ff,0,t,t,01,f,g,00094,0,-\n",
      "a,27.67,1.5,u,g,m,v,2,t,f,0,f,s,00368,0,-\n",
      "b,39.83,0.5,u,g,m,v,0.25,t,f,0,f,s,00288,0,-\n",
      "b,27.25,0.625,u,g,aa,v,0.455,t,f,0,t,g,00200,0,-\n",
      "b,37.17,4,u,g,c,bb,5,t,f,0,t,s,00280,0,-\n",
      "b,25.67,2.21,y,p,aa,v,4,t,f,0,f,g,00188,0,-\n",
      "b,34.00,4.5,u,g,aa,v,1,t,f,0,t,g,00240,0,-\n",
      "a,49.00,1.5,u,g,j,j,0,t,f,0,t,g,00100,27,-\n",
      "b,62.50,12.75,y,p,c,h,5,t,f,0,f,g,00112,0,-\n",
      "b,31.42,15.5,u,g,c,v,0.5,t,f,0,f,g,00120,0,-\n",
      "b,52.33,1.375,y,p,c,h,9.46,t,f,0,t,g,00200,100,-\n",
      "b,28.75,1.5,y,p,c,v,1.5,t,f,0,t,g,00000,225,-\n",
      "a,28.58,3.54,u,g,i,bb,0.5,t,f,0,t,g,00171,0,-\n",
      "b,23.00,0.625,y,p,aa,v,0.125,t,f,0,f,g,00180,1,-\n",
      "a,22.50,11,y,p,q,v,3,t,f,0,t,g,00268,0,-\n",
      "a,28.50,1,u,g,q,v,1,t,t,02,t,g,00167,500,-\n",
      "b,37.50,1.75,y,p,c,bb,0.25,t,f,0,t,g,00164,400,-\n",
      "b,35.25,16.5,y,p,c,v,4,t,f,0,f,g,00080,0,-\n",
      "b,18.67,5,u,g,q,v,0.375,t,t,02,f,g,00000,38,-\n",
      "b,25.00,12,u,g,k,v,2.25,t,t,02,t,g,00120,5,-\n",
      "b,27.83,4,y,p,i,h,5.75,t,t,02,t,g,00075,0,-\n",
      "b,54.83,15.5,u,g,e,z,0,t,t,20,f,g,00152,130,-\n",
      "b,28.75,1.165,u,g,k,v,0.5,t,f,0,f,s,00280,0,-\n",
      "a,25.00,11,y,p,aa,v,4.5,t,f,0,f,g,00120,0,-\n",
      "b,40.92,2.25,y,p,x,h,10,t,f,0,t,g,00176,0,-\n",
      "a,19.75,0.75,u,g,c,v,0.795,t,t,05,t,g,00140,5,-\n",
      "b,29.17,3.5,u,g,w,v,3.5,t,t,03,t,g,00329,0,-\n",
      "a,24.50,1.04,y,p,ff,ff,0.5,t,t,03,f,g,00180,147,-\n",
      "b,24.58,12.5,u,g,w,v,0.875,t,f,0,t,g,00260,0,-\n",
      "a,33.75,0.75,u,g,k,bb,1,t,t,03,t,g,00212,0,-\n",
      "b,20.67,1.25,y,p,c,h,1.375,t,t,03,t,g,00140,210,-\n",
      "a,25.42,1.125,u,g,q,v,1.29,t,t,02,f,g,00200,0,-\n",
      "b,37.75,7,u,g,q,h,11.5,t,t,07,t,g,00300,5,-\n",
      "b,52.50,6.5,u,g,k,v,6.29,t,t,15,f,g,00000,11202,+\n",
      "b,57.83,7.04,u,g,m,v,14,t,t,06,t,g,00360,1332,+\n",
      "a,20.75,10.335,u,g,cc,h,0.335,t,t,01,t,g,00080,50,+\n",
      "b,39.92,6.21,u,g,q,v,0.04,t,t,01,f,g,00200,300,+\n",
      "b,25.67,12.5,u,g,cc,v,1.21,t,t,67,t,g,00140,258,+\n",
      "a,24.75,12.5,u,g,aa,v,1.5,t,t,12,t,g,00120,567,+\n",
      "a,44.17,6.665,u,g,q,v,7.375,t,t,03,t,g,00000,0,+\n",
      "a,23.50,9,u,g,q,v,8.5,t,t,05,t,g,00120,0,+\n",
      "b,34.92,5,u,g,x,h,7.5,t,t,06,t,g,00000,1000,+\n",
      "b,47.67,2.5,u,g,m,bb,2.5,t,t,12,t,g,00410,2510,+\n",
      "b,22.75,11,u,g,q,v,2.5,t,t,07,t,g,00100,809,+\n",
      "b,34.42,4.25,u,g,i,bb,3.25,t,t,02,f,g,00274,610,+\n",
      "a,28.42,3.5,u,g,w,v,0.835,t,f,0,f,s,00280,0,+\n",
      "b,67.75,5.5,u,g,e,z,13,t,t,01,t,g,00000,0,+\n",
      "b,20.42,1.835,u,g,c,v,2.25,t,t,01,f,g,00100,150,+\n",
      "a,47.42,8,u,g,e,bb,6.5,t,t,06,f,g,00375,51100,+\n",
      "b,36.25,5,u,g,c,bb,2.5,t,t,06,f,g,00000,367,+\n",
      "b,32.67,5.5,u,g,q,h,5.5,t,t,12,t,g,00408,1000,+\n",
      "b,48.58,6.5,u,g,q,h,6,t,f,0,t,g,00350,0,+\n",
      "b,39.92,0.54,y,p,aa,v,0.5,t,t,03,f,g,00200,1000,+\n",
      "b,33.58,2.75,u,g,m,v,4.25,t,t,06,f,g,00204,0,+\n",
      "a,18.83,9.5,u,g,w,v,1.625,t,t,06,t,g,00040,600,+\n",
      "a,26.92,13.5,u,g,q,h,5,t,t,02,f,g,00000,5000,+\n",
      "a,31.25,3.75,u,g,cc,h,0.625,t,t,09,t,g,00181,0,+\n",
      "a,56.50,16,u,g,j,ff,0,t,t,15,f,g,00000,247,+\n",
      "b,43.00,0.29,y,p,cc,h,1.75,t,t,08,f,g,00100,375,+\n",
      "b,22.33,11,u,g,w,v,2,t,t,01,f,g,00080,278,+\n",
      "b,27.25,1.665,u,g,cc,h,5.085,t,t,09,f,g,00399,827,+\n",
      "b,32.83,2.5,u,g,cc,h,2.75,t,t,06,f,g,00160,2072,+\n",
      "b,23.25,1.5,u,g,q,v,2.375,t,t,03,t,g,00000,582,+\n",
      "a,40.33,7.54,y,p,q,h,8,t,t,14,f,g,00000,2300,+\n",
      "a,30.50,6.5,u,g,c,bb,4,t,t,07,t,g,00000,3065,+\n",
      "a,52.83,15,u,g,c,v,5.5,t,t,14,f,g,00000,2200,+\n",
      "a,46.67,0.46,u,g,cc,h,0.415,t,t,11,t,g,00440,6,+\n",
      "a,58.33,10,u,g,q,v,4,t,t,14,f,g,00000,1602,+\n",
      "b,37.33,6.5,u,g,m,h,4.25,t,t,12,t,g,00093,0,+\n",
      "b,23.08,2.5,u,g,c,v,1.085,t,t,11,t,g,00060,2184,+\n",
      "b,32.75,1.5,u,g,cc,h,5.5,t,t,03,t,g,00000,0,+\n",
      "a,21.67,11.5,y,p,j,j,0,t,t,11,t,g,00000,0,+\n",
      "a,28.50,3.04,y,p,x,h,2.54,t,t,01,f,g,00070,0,+\n",
      "a,68.67,15,u,g,e,z,0,t,t,14,f,g,00000,3376,+\n",
      "b,28.00,2,u,g,k,h,4.165,t,t,02,t,g,00181,0,+\n",
      "b,34.08,0.08,y,p,m,bb,0.04,t,t,01,t,g,00280,2000,+\n",
      "b,27.67,2,u,g,x,h,1,t,t,04,f,g,00140,7544,+\n",
      "b,44.00,2,u,g,m,v,1.75,t,t,02,t,g,00000,15,+\n",
      "b,25.08,1.71,u,g,x,v,1.665,t,t,01,t,g,00395,20,+\n",
      "b,32.00,1.75,y,p,e,h,0.04,t,f,0,t,g,00393,0,+\n",
      "a,60.58,16.5,u,g,q,v,11,t,f,0,t,g,00021,10561,+\n",
      "a,40.83,10,u,g,q,h,1.75,t,f,0,f,g,00029,837,+\n",
      "b,19.33,9.5,u,g,q,v,1,t,f,0,t,g,00060,400,+\n",
      "a,32.33,0.54,u,g,cc,v,0.04,t,f,0,f,g,00440,11177,+\n",
      "b,36.67,3.25,u,g,q,h,9,t,f,0,t,g,00102,639,+\n",
      "b,37.50,1.125,y,p,d,v,1.5,f,f,0,t,g,00431,0,+\n",
      "a,25.08,2.54,y,p,aa,v,0.25,t,f,0,t,g,00370,0,+\n",
      "b,41.33,0,u,g,c,bb,15,t,f,0,f,g,00000,0,+\n",
      "b,56.00,12.5,u,g,k,h,8,t,f,0,t,g,00024,2028,+\n",
      "a,49.83,13.585,u,g,k,h,8.5,t,f,0,t,g,00000,0,+\n",
      "b,22.67,10.5,u,g,q,h,1.335,t,f,0,f,g,00100,0,+\n",
      "b,27.00,1.5,y,p,w,v,0.375,t,f,0,t,g,00260,1065,+\n",
      "b,25.00,12.5,u,g,aa,v,3,t,f,0,t,s,00020,0,+\n",
      "a,26.08,8.665,u,g,aa,v,1.415,t,f,0,f,g,00160,150,+\n",
      "a,18.42,9.25,u,g,q,v,1.21,t,t,04,f,g,00060,540,+\n",
      "b,20.17,8.17,u,g,aa,v,1.96,t,t,14,f,g,00060,158,+\n",
      "b,47.67,0.29,u,g,c,bb,15,t,t,20,f,g,00000,15000,+\n",
      "a,21.25,2.335,u,g,i,bb,0.5,t,t,04,f,s,00080,0,+\n",
      "a,20.67,3,u,g,q,v,0.165,t,t,03,f,g,00100,6,+\n",
      "a,57.08,19.5,u,g,c,v,5.5,t,t,07,f,g,00000,3000,+\n",
      "a,22.42,5.665,u,g,q,v,2.585,t,t,07,f,g,00129,3257,+\n",
      "b,48.75,8.5,u,g,c,h,12.5,t,t,09,f,g,00181,1655,+\n",
      "b,40.00,6.5,u,g,aa,bb,3.5,t,t,01,f,g,00000,500,+\n",
      "b,40.58,5,u,g,c,v,5,t,t,07,f,g,00000,3065,+\n",
      "a,28.67,1.04,u,g,c,v,2.5,t,t,05,t,g,00300,1430,+\n",
      "a,33.08,4.625,u,g,q,h,1.625,t,t,02,f,g,00000,0,+\n",
      "b,21.33,10.5,u,g,c,v,3,t,f,0,t,g,00000,0,+\n",
      "b,42.00,0.205,u,g,i,h,5.125,t,f,0,f,g,00400,0,+\n",
      "b,41.75,0.96,u,g,x,v,2.5,t,f,0,f,g,00510,600,+\n",
      "b,22.67,1.585,y,p,w,v,3.085,t,t,06,f,g,00080,0,+\n",
      "b,34.50,4.04,y,p,i,bb,8.5,t,t,07,t,g,00195,0,+\n",
      "b,28.25,5.04,y,p,c,bb,1.5,t,t,08,t,g,00144,7,+\n",
      "b,33.17,3.165,y,p,x,v,3.165,t,t,03,t,g,00380,0,+\n",
      "b,48.17,7.625,u,g,w,h,15.5,t,t,12,f,g,00000,790,+\n",
      "b,27.58,2.04,y,p,aa,v,2,t,t,03,t,g,00370,560,+\n",
      "b,22.58,10.04,u,g,x,v,0.04,t,t,09,f,g,00060,396,+\n",
      "a,24.08,0.5,u,g,q,h,1.25,t,t,01,f,g,00000,678,+\n",
      "a,41.33,1,u,g,i,bb,2.25,t,f,0,t,g,00000,300,+\n",
      "a,20.75,10.25,u,g,q,v,0.71,t,t,02,t,g,00049,0,+\n",
      "b,36.33,2.125,y,p,w,v,0.085,t,t,01,f,g,00050,1187,+\n",
      "a,35.42,12,u,g,q,h,14,t,t,08,f,g,00000,6590,+\n",
      "b,28.67,9.335,u,g,q,h,5.665,t,t,06,f,g,00381,168,+\n",
      "b,35.17,2.5,u,g,k,v,4.5,t,t,07,f,g,00150,1270,+\n",
      "b,39.50,4.25,u,g,c,bb,6.5,t,t,16,f,g,00117,1210,+\n",
      "b,39.33,5.875,u,g,cc,h,10,t,t,14,t,g,00399,0,+\n",
      "b,24.33,6.625,y,p,d,v,5.5,t,f,0,t,s,00100,0,+\n",
      "b,60.08,14.5,u,g,ff,ff,18,t,t,15,t,g,00000,1000,+\n",
      "b,23.08,11.5,u,g,i,v,3.5,t,t,09,f,g,00056,742,+\n",
      "b,26.67,2.71,y,p,cc,v,5.25,t,t,01,f,g,00211,0,+\n",
      "b,48.17,3.5,u,g,aa,v,3.5,t,f,0,f,s,00230,0,+\n",
      "b,41.17,4.04,u,g,cc,h,7,t,t,08,f,g,00320,0,+\n",
      "b,55.92,11.5,u,g,ff,ff,5,t,t,05,f,g,00000,8851,+\n",
      "b,53.92,9.625,u,g,e,v,8.665,t,t,05,f,g,00000,0,+\n",
      "a,18.92,9.25,y,p,c,v,1,t,t,04,t,g,00080,500,+\n",
      "a,50.08,12.54,u,g,aa,v,2.29,t,t,03,t,g,00156,0,+\n",
      "b,65.42,11,u,g,e,z,20,t,t,07,t,g,00022,0,+\n",
      "a,17.58,9,u,g,aa,v,1.375,t,f,0,t,g,00000,0,+\n",
      "a,18.83,9.54,u,g,aa,v,0.085,t,f,0,f,g,00100,0,+\n",
      "a,37.75,5.5,u,g,q,v,0.125,t,f,0,t,g,00228,0,+\n",
      "b,23.25,4,u,g,c,bb,0.25,t,f,0,t,g,00160,0,+\n",
      "b,18.08,5.5,u,g,k,v,0.5,t,f,0,f,g,00080,0,+\n",
      "a,22.50,8.46,y,p,x,v,2.46,f,f,0,f,g,00164,0,+\n",
      "b,19.67,0.375,u,g,q,v,2,t,t,02,t,g,00080,0,+\n",
      "b,22.08,11,u,g,cc,v,0.665,t,f,0,f,g,00100,0,+\n",
      "b,25.17,3.5,u,g,cc,v,0.625,t,t,07,f,g,00000,7059,+\n",
      "a,47.42,3,u,g,x,v,13.875,t,t,02,t,g,00519,1704,+\n",
      "b,33.50,1.75,u,g,x,h,4.5,t,t,04,t,g,00253,857,+\n",
      "b,27.67,13.75,u,g,w,v,5.75,t,f,0,t,g,00487,500,+\n",
      "a,58.42,21,u,g,i,bb,10,t,t,13,f,g,00000,6700,+\n",
      "a,20.67,1.835,u,g,q,v,2.085,t,t,05,f,g,00220,2503,+\n",
      "b,26.17,0.25,u,g,i,bb,0,t,f,0,t,g,00000,0,+\n",
      "b,21.33,7.5,u,g,aa,v,1.415,t,t,01,f,g,00080,9800,+\n",
      "b,42.83,4.625,u,g,q,v,4.58,t,f,0,f,s,00000,0,+\n",
      "b,38.17,10.125,u,g,x,v,2.5,t,t,06,f,g,00520,196,+\n",
      "b,20.50,10,y,p,c,v,2.5,t,f,0,f,s,00040,0,+\n",
      "b,48.25,25.085,u,g,w,v,1.75,t,t,03,f,g,00120,14,+\n",
      "b,28.33,5,u,g,w,v,11,t,f,0,t,g,00070,0,+\n",
      "b,18.50,2,u,g,i,v,1.5,t,t,02,f,g,00120,300,+\n",
      "b,33.17,3.04,y,p,c,h,2.04,t,t,01,t,g,00180,18027,+\n",
      "b,45.00,8.5,u,g,cc,h,14,t,t,01,t,g,00088,2000,+\n",
      "a,19.67,0.21,u,g,q,h,0.29,t,t,11,f,g,00080,99,+\n",
      "b,21.83,11,u,g,x,v,0.29,t,t,06,f,g,00121,0,+\n",
      "b,40.25,21.5,u,g,e,z,20,t,t,11,f,g,00000,1200,+\n",
      "b,41.42,5,u,g,q,h,5,t,t,06,t,g,00470,0,+\n",
      "a,17.83,11,u,g,x,h,1,t,t,11,f,g,00000,3000,+\n",
      "b,23.17,11.125,u,g,x,h,0.46,t,t,01,f,g,00100,0,+\n",
      "b,18.17,10.25,u,g,c,h,1.085,f,f,0,f,g,00320,13,-\n",
      "b,20.00,11.045,u,g,c,v,2,f,f,0,t,g,00136,0,-\n",
      "b,20.00,0,u,g,d,v,0.5,f,f,0,f,g,00144,0,-\n",
      "a,20.75,9.54,u,g,i,v,0.04,f,f,0,f,g,00200,1000,-\n",
      "a,24.50,1.75,y,p,c,v,0.165,f,f,0,f,g,00132,0,-\n",
      "b,32.75,2.335,u,g,d,h,5.75,f,f,0,t,g,00292,0,-\n",
      "a,52.17,0,y,p,ff,ff,0,f,f,0,f,g,00000,0,-\n",
      "a,48.17,1.335,u,g,i,o,0.335,f,f,0,f,g,00000,120,-\n",
      "a,20.42,10.5,y,p,x,h,0,f,f,0,t,g,00154,32,-\n",
      "b,50.75,0.585,u,g,ff,ff,0,f,f,0,f,g,00145,0,-\n",
      "b,17.08,0.085,y,p,c,v,0.04,f,f,0,f,g,00140,722,-\n",
      "b,18.33,1.21,y,p,e,dd,0,f,f,0,f,g,00100,0,-\n",
      "a,32.00,6,u,g,d,v,1.25,f,f,0,f,g,00272,0,-\n",
      "b,59.67,1.54,u,g,q,v,0.125,t,f,0,t,g,00260,0,+\n",
      "b,18.00,0.165,u,g,q,n,0.21,f,f,0,f,g,00200,40,+\n",
      "b,32.33,2.5,u,g,c,v,1.25,f,f,0,t,g,00280,0,-\n",
      "b,18.08,6.75,y,p,m,v,0.04,f,f,0,f,g,00140,0,-\n",
      "b,38.25,10.125,y,p,k,v,0.125,f,f,0,f,g,00160,0,-\n",
      "b,30.67,2.5,u,g,cc,h,2.25,f,f,0,t,s,00340,0,-\n",
      "b,18.58,5.71,u,g,d,v,0.54,f,f,0,f,g,00120,0,-\n",
      "a,19.17,5.415,u,g,i,h,0.29,f,f,0,f,g,00080,484,-\n",
      "a,18.17,10,y,p,q,h,0.165,f,f,0,f,g,00340,0,-\n",
      "b,16.25,0.835,u,g,m,v,0.085,t,f,0,f,s,00200,0,-\n",
      "b,21.17,0.875,y,p,c,h,0.25,f,f,0,f,g,00280,204,-\n",
      "b,23.92,0.585,y,p,cc,h,0.125,f,f,0,f,g,00240,1,-\n",
      "b,17.67,4.46,u,g,c,v,0.25,f,f,0,f,s,00080,0,-\n",
      "a,16.50,1.25,u,g,q,v,0.25,f,t,01,f,g,00108,98,-\n",
      "b,23.25,12.625,u,g,c,v,0.125,f,t,02,f,g,00000,5552,-\n",
      "b,17.58,10,u,g,w,h,0.165,f,t,01,f,g,00120,1,-\n",
      "b,29.50,0.58,u,g,w,v,0.29,f,t,01,f,g,00340,2803,-\n",
      "b,18.83,0.415,y,p,c,v,0.165,f,t,01,f,g,00200,1,-\n",
      "a,21.75,1.75,y,p,j,j,0,f,f,0,f,g,00160,0,-\n",
      "b,23.00,0.75,u,g,m,v,0.5,f,f,0,t,s,00320,0,-\n",
      "a,18.25,10,u,g,w,v,1,f,t,01,f,g,00120,1,-\n",
      "b,25.42,0.54,u,g,w,v,0.165,f,t,01,f,g,00272,444,-\n",
      "b,35.75,2.415,u,g,w,v,0.125,f,t,02,f,g,00220,1,-\n",
      "a,16.08,0.335,u,g,ff,ff,0,f,t,01,f,g,00160,126,-\n",
      "a,31.92,3.125,u,g,ff,ff,3.04,f,t,02,t,g,00200,4,-\n",
      "b,69.17,9,u,g,ff,ff,4,f,t,01,f,g,00070,6,-\n",
      "b,32.92,2.5,u,g,aa,v,1.75,f,t,02,t,g,00720,0,-\n",
      "b,16.33,2.75,u,g,aa,v,0.665,f,t,01,f,g,00080,21,-\n",
      "b,22.17,12.125,u,g,c,v,3.335,f,t,02,t,g,00180,173,-\n",
      "a,57.58,2,u,g,ff,ff,6.5,f,t,01,f,g,00000,10,-\n",
      "b,18.25,0.165,u,g,d,v,0.25,f,f,0,t,s,00280,0,-\n",
      "b,23.42,1,u,g,c,v,0.5,f,f,0,t,s,00280,0,-\n",
      "a,15.92,2.875,u,g,q,v,0.085,f,f,0,f,g,00120,0,-\n",
      "a,24.75,13.665,u,g,q,h,1.5,f,f,0,f,g,00280,1,-\n",
      "b,48.75,26.335,y,p,ff,ff,0,t,f,0,t,g,00000,0,-\n",
      "b,23.50,2.75,u,g,ff,ff,4.5,f,f,0,f,g,00160,25,-\n",
      "b,18.58,10.29,u,g,ff,ff,0.415,f,f,0,f,g,00080,0,-\n",
      "b,27.75,1.29,u,g,k,h,0.25,f,f,0,t,s,00140,0,-\n",
      "a,31.75,3,y,p,j,j,0,f,f,0,f,g,00160,20,-\n",
      "a,24.83,4.5,u,g,w,v,1,f,f,0,t,g,00360,6,-\n",
      "b,19.00,1.75,y,p,c,v,2.335,f,f,0,t,g,00112,6,-\n",
      "a,16.33,0.21,u,g,aa,v,0.125,f,f,0,f,g,00200,1,-\n",
      "a,18.58,10,u,g,d,v,0.415,f,f,0,f,g,00080,42,-\n",
      "b,16.25,0,y,p,aa,v,0.25,f,f,0,f,g,00060,0,-\n",
      "b,23.00,0.75,u,g,m,v,0.5,t,f,0,t,s,00320,0,-\n",
      "b,21.17,0.25,y,p,c,h,0.25,f,f,0,f,g,00280,204,-\n",
      "b,17.50,22,l,gg,ff,o,0,f,f,0,t,p,00450,100000,+\n",
      "b,19.17,0,y,p,m,bb,0,f,f,0,t,s,00500,1,+\n",
      "b,36.75,0.125,y,p,c,v,1.5,f,f,0,t,g,00232,113,+\n",
      "b,21.25,1.5,u,g,w,v,1.5,f,f,0,f,g,00150,8,+\n",
      "a,18.08,0.375,l,gg,cc,ff,10,f,f,0,t,s,00300,0,+\n",
      "a,33.67,0.375,u,g,cc,v,0.375,f,f,0,f,g,00300,44,+\n",
      "b,48.58,0.205,y,p,k,v,0.25,t,t,11,f,g,00380,2732,+\n",
      "b,33.67,1.25,u,g,w,v,1.165,f,f,0,f,g,00120,0,-\n",
      "a,29.50,1.085,y,p,x,v,1,f,f,0,f,g,00280,13,-\n",
      "b,30.17,1.085,y,p,c,v,0.04,f,f,0,f,g,00170,179,-\n",
      "b,34.83,2.5,y,p,w,v,3,f,f,0,f,s,00200,0,-\n",
      "a,33.25,2.5,y,p,c,v,2.5,f,f,0,t,g,00000,2,-\n",
      "b,34.08,2.5,u,g,c,v,1,f,f,0,f,g,00460,16,-\n",
      "a,25.25,12.5,u,g,d,v,1,f,f,0,t,g,00180,1062,-\n",
      "b,34.75,2.5,u,g,cc,bb,0.5,f,f,0,f,g,00348,0,-\n",
      "b,27.67,0.75,u,g,q,h,0.165,f,f,0,t,g,00220,251,-\n",
      "b,47.33,6.5,u,g,c,v,1,f,f,0,t,g,00000,228,-\n",
      "a,34.83,1.25,y,p,i,h,0.5,f,f,0,t,g,00160,0,-\n",
      "a,33.25,3,y,p,aa,v,2,f,f,0,f,g,00180,0,-\n",
      "b,28.00,3,u,g,w,v,0.75,f,f,0,t,g,00300,67,-\n",
      "a,39.08,4,u,g,c,v,3,f,f,0,f,g,00480,0,-\n",
      "b,42.75,4.085,u,g,aa,v,0.04,f,f,0,f,g,00108,100,-\n",
      "b,26.92,2.25,u,g,i,bb,0.5,f,f,0,t,g,00640,4000,-\n",
      "b,33.75,2.75,u,g,i,bb,0,f,f,0,f,g,00180,0,-\n",
      "b,38.92,1.75,u,g,k,v,0.5,f,f,0,t,g,00300,2,-\n",
      "b,62.75,7,u,g,e,z,0,f,f,0,f,g,00000,12,-\n",
      "b,26.75,4.5,y,p,c,bb,2.5,f,f,0,f,g,00200,1210,-\n",
      "b,63.33,0.54,u,g,c,v,0.585,t,t,03,t,g,00180,0,-\n",
      "b,27.83,1.5,u,g,w,v,2.25,f,t,01,t,g,00100,3,-\n",
      "a,26.17,2,u,g,j,j,0,f,f,0,t,g,00276,1,-\n",
      "b,22.17,0.585,y,p,ff,ff,0,f,f,0,f,g,00100,0,-\n",
      "b,22.50,11.5,y,p,m,v,1.5,f,f,0,t,g,00000,4000,-\n",
      "b,30.75,1.585,u,g,d,v,0.585,f,f,0,t,s,00000,0,-\n",
      "b,36.67,2,u,g,i,v,0.25,f,f,0,t,g,00221,0,-\n",
      "a,16.00,0.165,u,g,aa,v,1,f,t,02,t,g,00320,1,-\n",
      "b,41.17,1.335,u,g,d,v,0.165,f,f,0,f,g,00168,0,-\n",
      "a,19.50,0.165,u,g,q,v,0.04,f,f,0,t,g,00380,0,-\n",
      "b,32.42,3,u,g,d,v,0.165,f,f,0,t,g,00120,0,-\n",
      "a,36.75,4.71,u,g,ff,ff,0,f,f,0,f,g,00160,0,-\n",
      "a,30.25,5.5,u,g,k,v,5.5,f,f,0,t,s,00100,0,-\n",
      "b,23.08,2.5,u,g,ff,ff,0.085,f,f,0,t,g,00100,4208,-\n",
      "b,26.83,0.54,u,g,k,ff,0,f,f,0,f,g,00100,0,-\n",
      "b,16.92,0.335,y,p,k,v,0.29,f,f,0,f,s,00200,0,-\n",
      "b,24.42,2,u,g,e,dd,0.165,f,t,02,f,g,00320,1300,-\n",
      "b,42.83,1.25,u,g,m,v,13.875,f,t,01,t,g,00352,112,-\n",
      "a,22.75,6.165,u,g,aa,v,0.165,f,f,0,f,g,00220,1000,-\n",
      "b,39.42,1.71,y,p,m,v,0.165,f,f,0,f,s,00400,0,-\n",
      "a,23.58,11.5,y,p,k,h,3,f,f,0,t,g,00020,16,-\n",
      "b,21.42,0.75,y,p,r,n,0.75,f,f,0,t,g,00132,2,-\n",
      "b,33.00,2.5,y,p,w,v,7,f,f,0,t,g,00280,0,-\n",
      "b,26.33,13,u,g,e,dd,0,f,f,0,t,g,00140,1110,-\n",
      "a,45.00,4.585,u,g,k,h,1,f,f,0,t,s,00240,0,-\n",
      "b,26.25,1.54,u,g,w,v,0.125,f,f,0,f,g,00100,0,-\n",
      "a,20.83,0.5,y,p,e,dd,1,f,f,0,f,g,00260,0,-\n",
      "b,28.67,14.5,u,g,d,v,0.125,f,f,0,f,g,00000,286,-\n",
      "b,20.67,0.835,y,p,c,v,2,f,f,0,t,s,00240,0,-\n",
      "b,34.42,1.335,u,g,i,bb,0.125,f,f,0,t,g,00440,4500,-\n",
      "b,33.58,0.25,u,g,i,bb,4,f,f,0,t,s,00420,0,-\n",
      "b,43.17,5,u,g,i,bb,2.25,f,f,0,t,g,00141,0,-\n",
      "a,22.67,7,u,g,c,v,0.165,f,f,0,f,g,00160,0,-\n",
      "a,24.33,2.5,y,p,i,bb,4.5,f,f,0,f,g,00200,456,-\n",
      "a,56.83,4.25,y,p,ff,ff,5,f,f,0,t,g,00000,4,-\n",
      "b,22.08,11.46,u,g,k,v,1.585,f,f,0,t,g,00100,1212,-\n",
      "b,34.00,5.5,y,p,c,v,1.5,f,f,0,t,g,00060,0,-\n",
      "b,22.58,1.5,y,p,aa,v,0.54,f,f,0,t,g,00120,67,-\n",
      "b,21.17,0,u,g,c,v,0.5,f,f,0,t,s,00000,0,-\n",
      "b,26.67,14.585,u,g,i,bb,0,f,f,0,t,g,00178,0,-\n",
      "b,22.92,0.17,u,g,m,v,0.085,f,f,0,f,s,00000,0,-\n",
      "b,15.17,7,u,g,e,v,1,f,f,0,f,g,00600,0,-\n",
      "b,39.92,5,u,g,i,bb,0.21,f,f,0,f,g,00550,0,-\n",
      "b,27.42,12.5,u,g,aa,bb,0.25,f,f,0,t,g,00720,0,-\n",
      "b,24.75,0.54,u,g,m,v,1,f,f,0,t,g,00120,1,-\n",
      "b,41.17,1.25,y,p,w,v,0.25,f,f,0,f,g,00000,195,-\n",
      "a,33.08,1.625,u,g,d,v,0.54,f,f,0,t,g,00000,0,-\n",
      "b,29.83,2.04,y,p,x,h,0.04,f,f,0,f,g,00128,1,-\n",
      "a,23.58,0.585,y,p,ff,ff,0.125,f,f,0,f,g,00120,87,-\n",
      "b,26.17,12.5,y,p,k,h,1.25,f,f,0,t,g,00000,17,-\n",
      "b,31.00,2.085,u,g,c,v,0.085,f,f,0,f,g,00300,0,-\n",
      "b,20.75,5.085,y,p,j,v,0.29,f,f,0,f,g,00140,184,-\n",
      "b,28.92,0.375,u,g,c,v,0.29,f,f,0,f,g,00220,140,-\n",
      "a,51.92,6.5,u,g,i,bb,3.085,f,f,0,t,g,00073,0,-\n",
      "a,22.67,0.335,u,g,q,v,0.75,f,f,0,f,s,00160,0,-\n",
      "b,34.00,5.085,y,p,i,bb,1.085,f,f,0,t,g,00480,0,-\n",
      "a,69.50,6,u,g,ff,ff,0,f,f,0,f,s,00000,0,-\n",
      "a,19.58,0.665,y,p,c,v,1,f,t,01,f,g,02000,2,-\n",
      "b,16.00,3.125,u,g,w,v,0.085,f,t,01,f,g,00000,6,-\n",
      "b,17.08,0.25,u,g,q,v,0.335,f,t,04,f,g,00160,8,-\n",
      "b,31.25,2.835,u,g,ff,ff,0,f,t,05,f,g,00176,146,-\n",
      "b,25.17,3,u,g,c,v,1.25,f,t,01,f,g,00000,22,-\n",
      "a,22.67,0.79,u,g,i,v,0.085,f,f,0,f,g,00144,0,-\n",
      "b,40.58,1.5,u,g,i,bb,0,f,f,0,f,s,00300,0,-\n",
      "b,22.25,0.46,u,g,k,v,0.125,f,f,0,t,g,00280,55,-\n",
      "a,22.25,1.25,y,p,ff,ff,3.25,f,f,0,f,g,00280,0,-\n",
      "b,22.50,0.125,y,p,k,v,0.125,f,f,0,f,g,00200,70,-\n",
      "b,23.58,1.79,u,g,c,v,0.54,f,f,0,t,g,00136,1,-\n",
      "b,38.42,0.705,u,g,c,v,0.375,f,t,02,f,g,00225,500,-\n",
      "a,26.58,2.54,y,p,ff,ff,0,f,f,0,t,g,00180,60,-\n",
      "b,35.00,2.5,u,g,i,v,1,f,f,0,t,g,00210,0,-\n",
      "b,20.42,1.085,u,g,q,v,1.5,f,f,0,f,g,00108,7,-\n",
      "b,29.42,1.25,u,g,w,v,1.75,f,f,0,f,g,00200,0,-\n",
      "b,26.17,0.835,u,g,cc,v,1.165,f,f,0,f,g,00100,0,-\n",
      "b,33.67,2.165,u,g,c,v,1.5,f,f,0,f,p,00120,0,-\n",
      "b,24.58,1.25,u,g,c,v,0.25,f,f,0,f,g,00110,0,-\n",
      "a,27.67,2.04,u,g,w,v,0.25,f,f,0,t,g,00180,50,-\n",
      "b,37.50,0.835,u,g,e,v,0.04,f,f,0,f,g,00120,5,-\n",
      "b,49.17,2.29,u,g,ff,ff,0.29,f,f,0,f,g,00200,3,-\n",
      "b,33.58,0.335,y,p,cc,v,0.085,f,f,0,f,g,00180,0,-\n",
      "b,51.83,3,y,p,ff,ff,1.5,f,f,0,f,g,00180,4,-\n",
      "b,22.92,3.165,y,p,c,v,0.165,f,f,0,f,g,00160,1058,-\n",
      "b,21.83,1.54,u,g,k,v,0.085,f,f,0,t,g,00356,0,-\n",
      "b,25.25,1,u,g,aa,v,0.5,f,f,0,f,g,00200,0,-\n",
      "b,58.58,2.71,u,g,c,v,2.415,f,f,0,t,g,00320,0,-\n",
      "b,19.00,0,y,p,ff,ff,0,f,t,04,f,g,00045,1,-\n",
      "b,19.58,0.585,u,g,ff,ff,0,f,t,03,f,g,00350,769,-\n",
      "a,53.33,0.165,u,g,ff,ff,0,f,f,0,t,s,00062,27,-\n",
      "a,27.17,1.25,u,g,ff,ff,0,f,t,01,f,g,00092,300,-\n",
      "b,25.92,0.875,u,g,k,v,0.375,f,t,02,t,g,00174,3,-\n",
      "b,23.08,0,u,g,k,v,1,f,t,11,f,s,00000,0,-\n",
      "b,39.58,5,u,g,ff,ff,0,f,t,02,f,g,00017,1,-\n",
      "b,30.58,2.71,y,p,m,v,0.125,f,f,0,t,s,00080,0,-\n",
      "b,17.25,3,u,g,k,v,0.04,f,f,0,t,g,00160,40,-\n",
      "a,17.67,0,y,p,j,ff,0,f,f,0,f,g,00086,0,-\n",
      "b,16.50,0.125,u,g,c,v,0.165,f,f,0,f,g,00132,0,-\n",
      "a,27.33,1.665,u,g,ff,ff,0,f,f,0,f,g,00340,1,-\n",
      "b,31.25,1.125,u,g,ff,ff,0,f,t,01,f,g,00096,19,-\n",
      "b,20.00,7,u,g,c,v,0.5,f,f,0,f,g,00000,0,-\n",
      "b,39.50,1.625,u,g,c,v,1.5,f,f,0,f,g,00000,316,-\n",
      "b,36.50,4.25,u,g,q,v,3.5,f,f,0,f,g,00454,50,-\n",
      "b,52.42,1.5,u,g,d,v,3.75,f,f,0,t,g,00000,350,-\n",
      "b,36.17,18.125,u,g,w,v,0.085,f,f,0,f,g,00320,3552,-\n",
      "b,29.67,0.75,y,p,c,v,0.04,f,f,0,f,g,00240,0,-\n",
      "b,36.17,5.5,u,g,i,bb,5,f,f,0,f,g,00210,687,-\n",
      "b,25.67,0.29,y,p,c,v,1.5,f,f,0,t,g,00160,0,-\n",
      "a,24.50,2.415,y,p,c,v,0,f,f,0,f,g,00120,0,-\n",
      "b,24.08,0.875,u,g,m,v,0.085,f,t,04,f,g,00254,1950,-\n",
      "b,21.92,0.5,u,g,c,v,0.125,f,f,0,f,g,00360,0,-\n",
      "a,36.58,0.29,u,g,ff,ff,0,f,t,10,f,g,00200,18,-\n",
      "a,23.00,1.835,u,g,j,j,0,f,t,01,f,g,00200,53,-\n",
      "a,27.58,3,u,g,m,v,2.79,f,t,01,t,g,00280,10,-\n",
      "b,31.08,3.085,u,g,c,v,2.5,f,t,02,t,g,00160,41,-\n",
      "a,30.42,1.375,u,g,w,h,0.04,f,t,03,f,g,00000,33,-\n",
      "b,22.08,2.335,u,g,k,v,0.75,f,f,0,f,g,00180,0,-\n",
      "b,16.33,4.085,u,g,i,h,0.415,f,f,0,t,g,00120,0,-\n",
      "a,21.92,11.665,u,g,k,h,0.085,f,f,0,f,g,00320,5,-\n",
      "b,21.08,4.125,y,p,i,h,0.04,f,f,0,f,g,00140,100,-\n",
      "b,17.42,6.5,u,g,i,v,0.125,f,f,0,f,g,00060,100,-\n",
      "b,19.17,4,y,p,i,v,1,f,f,0,t,g,00360,1000,-\n",
      "b,20.67,0.415,u,g,c,v,0.125,f,f,0,f,g,00000,44,-\n",
      "b,26.75,2,u,g,d,v,0.75,f,f,0,t,g,00080,0,-\n",
      "b,23.58,0.835,u,g,i,h,0.085,f,f,0,t,g,00220,5,-\n",
      "b,39.17,2.5,y,p,i,h,10,f,f,0,t,s,00200,0,-\n",
      "b,22.75,11.5,u,g,i,v,0.415,f,f,0,f,g,00000,0,-\n",
      "a,16.92,0.5,u,g,i,v,0.165,f,t,06,t,g,00240,35,-\n",
      "b,23.50,3.165,y,p,k,v,0.415,f,t,01,t,g,00280,80,-\n",
      "a,17.33,9.5,u,g,aa,v,1.75,f,t,10,t,g,00000,10,-\n",
      "b,23.75,0.415,y,p,c,v,0.04,f,t,02,f,g,00128,6,-\n",
      "b,34.67,1.08,u,g,m,v,1.165,f,f,0,f,s,00028,0,-\n",
      "b,74.83,19,y,p,ff,ff,0.04,f,t,02,f,g,00000,351,-\n",
      "b,28.17,0.125,y,p,k,v,0.085,f,f,0,f,g,00216,2100,-\n",
      "b,24.50,13.335,y,p,aa,v,0.04,f,f,0,t,g,00120,475,-\n",
      "b,18.83,3.54,y,p,ff,ff,0,f,f,0,t,g,00180,1,-\n",
      "a,47.25,0.75,u,g,q,h,2.75,t,t,01,f,g,00333,892,+\n",
      "b,24.17,0.875,u,g,q,v,4.625,t,t,02,t,g,00520,2000,+\n",
      "b,39.25,9.5,u,g,m,v,6.5,t,t,14,f,g,00240,4607,+\n",
      "a,20.50,11.835,u,g,c,h,6,t,f,0,f,g,00340,0,+\n",
      "a,18.83,4.415,y,p,c,h,3,t,f,0,f,g,00240,0,+\n",
      "b,19.17,9.5,u,g,w,v,1.5,t,f,0,f,g,00120,2206,+\n",
      "a,25.00,0.875,u,g,x,h,1.04,t,f,0,t,g,00160,5860,+\n",
      "b,20.17,9.25,u,g,c,v,1.665,t,t,03,t,g,00040,28,+\n",
      "b,25.75,0.5,u,g,c,v,1.46,t,t,05,t,g,00312,0,+\n",
      "b,20.42,7,u,g,c,v,1.625,t,t,03,f,g,00200,1391,+\n",
      "b,39.00,5,u,g,cc,v,3.5,t,t,10,t,g,00000,0,+\n",
      "a,64.08,0.165,u,g,ff,ff,0,t,t,01,f,g,00232,100,+\n",
      "b,28.25,5.125,u,g,x,v,4.75,t,t,02,f,g,00420,7,+\n",
      "a,28.75,3.75,u,g,c,v,1.085,t,t,01,t,g,00371,0,+\n",
      "b,31.33,19.5,u,g,c,v,7,t,t,16,f,g,00000,5000,+\n",
      "a,18.92,9,u,g,aa,v,0.75,t,t,02,f,g,00088,591,+\n",
      "a,24.75,3,u,g,q,h,1.835,t,t,19,f,g,00000,500,+\n",
      "a,30.67,12,u,g,c,v,2,t,t,01,f,g,00220,19,+\n",
      "b,21.00,4.79,y,p,w,v,2.25,t,t,01,t,g,00080,300,+\n",
      "b,13.75,4,y,p,w,v,1.75,t,t,02,t,g,00120,1000,+\n",
      "a,46.00,4,u,g,j,j,0,t,f,0,f,g,00100,960,+\n",
      "a,44.33,0,u,g,c,v,2.5,t,f,0,f,g,00000,0,+\n",
      "b,20.25,9.96,u,g,e,dd,0,t,f,0,f,g,00000,0,+\n",
      "b,22.67,2.54,y,p,c,h,2.585,t,f,0,f,g,00000,0,+\n",
      "a,60.92,5,u,g,aa,v,4,t,t,04,f,g,00000,99,+\n",
      "b,16.08,0.75,u,g,c,v,1.75,t,t,05,t,g,00352,690,+\n",
      "a,28.17,0.375,u,g,q,v,0.585,t,t,04,f,g,00080,0,+\n",
      "b,39.17,1.71,u,g,x,v,0.125,t,t,05,t,g,00480,0,+\n",
      "a,30.00,5.29,u,g,e,dd,2.25,t,t,05,t,g,00099,500,+\n",
      "b,22.83,3,u,g,m,v,1.29,t,t,01,f,g,00260,800,+\n",
      "a,22.50,8.5,u,g,q,v,1.75,t,t,10,f,g,00080,990,-\n",
      "a,28.58,1.665,u,g,q,v,2.415,t,f,0,t,g,00440,0,-\n",
      "b,45.17,1.5,u,g,c,v,2.5,t,f,0,t,g,00140,0,-\n",
      "b,41.58,1.75,u,g,k,v,0.21,t,f,0,f,g,00160,0,-\n",
      "a,57.08,0.335,u,g,i,bb,1,t,f,0,t,g,00252,2197,-\n",
      "a,55.75,7.08,u,g,k,h,6.75,t,t,03,t,g,00100,50,-\n",
      "b,43.25,25.21,u,g,q,h,0.21,t,t,01,f,g,00760,90,-\n",
      "a,25.33,2.085,u,g,c,h,2.75,t,f,0,t,g,00360,1,-\n",
      "a,24.58,0.67,u,g,aa,h,1.75,t,f,0,f,g,00400,0,-\n",
      "b,43.17,2.25,u,g,i,bb,0.75,t,f,0,f,g,00560,0,-\n",
      "b,40.92,0.835,u,g,ff,ff,0,t,f,0,f,g,00130,1,-\n",
      "b,31.83,2.5,u,g,aa,v,7.5,t,f,0,t,g,00523,0,-\n",
      "a,33.92,1.585,y,p,ff,ff,0,t,f,0,f,g,00320,0,-\n",
      "a,24.92,1.25,u,g,ff,ff,0,t,f,0,f,g,00080,0,-\n",
      "b,35.25,3.165,u,g,x,h,3.75,t,f,0,t,g,00680,0,-\n",
      "b,34.25,1.75,u,g,w,bb,0.25,t,f,0,t,g,00163,0,-\n",
      "b,19.42,1.5,y,p,cc,v,2,t,f,0,t,g,00100,20,-\n",
      "b,42.75,3,u,g,i,bb,1,t,f,0,f,g,00000,200,-\n",
      "b,19.67,10,y,p,k,h,0.835,t,f,0,t,g,00140,0,-\n",
      "b,36.33,3.79,u,g,w,v,1.165,t,f,0,t,g,00200,0,-\n",
      "b,30.08,1.04,y,p,i,bb,0.5,t,t,10,t,g,00132,28,-\n",
      "b,44.25,11,y,p,d,v,1.5,t,f,0,f,s,00000,0,-\n",
      "b,23.58,0.46,y,p,w,v,2.625,t,t,06,t,g,00208,347,-\n",
      "b,23.92,1.5,u,g,d,h,1.875,t,t,06,f,g,00200,327,+\n",
      "b,33.17,1,u,g,x,v,0.75,t,t,07,t,g,00340,4071,+\n",
      "b,48.33,12,u,g,m,v,16,t,f,0,f,s,00110,0,+\n",
      "b,76.75,22.29,u,g,e,z,12.75,t,t,01,t,g,00000,109,+\n",
      "b,51.33,10,u,g,i,bb,0,t,t,11,f,g,00000,1249,+\n",
      "b,34.75,15,u,g,r,n,5.375,t,t,09,t,g,00000,134,+\n",
      "b,38.58,3.335,u,g,w,v,4,t,t,14,f,g,00383,1344,+\n",
      "a,22.42,11.25,y,p,x,h,0.75,t,t,04,f,g,00000,321,+\n",
      "b,41.92,0.42,u,g,c,h,0.21,t,t,06,f,g,00220,948,+\n",
      "b,29.58,4.5,u,g,w,v,7.5,t,t,02,t,g,00330,0,+\n",
      "a,32.17,1.46,u,g,w,v,1.085,t,t,16,f,g,00120,2079,+\n",
      "b,51.42,0.04,u,g,x,h,0.04,t,f,0,f,g,00000,3000,+\n",
      "a,22.83,2.29,u,g,q,h,2.29,t,t,07,t,g,00140,2384,+\n",
      "a,25.00,12.33,u,g,cc,h,3.5,t,t,06,f,g,00400,458,+\n",
      "b,26.75,1.125,u,g,x,h,1.25,t,f,0,f,g,00000,5298,+\n",
      "b,23.33,1.5,u,g,c,h,1.415,t,f,0,f,g,00422,200,+\n",
      "b,24.42,12.335,u,g,q,h,1.585,t,f,0,t,g,00120,0,+\n",
      "b,42.17,5.04,u,g,q,h,12.75,t,f,0,t,g,00092,0,+\n",
      "a,20.83,3,u,g,aa,v,0.04,t,f,0,f,g,00100,0,+\n",
      "b,23.08,11.5,u,g,w,h,2.125,t,t,11,t,g,00290,284,+\n",
      "a,25.17,2.875,u,g,x,h,0.875,t,f,0,f,g,00360,0,+\n",
      "b,43.08,0.375,y,p,c,v,0.375,t,t,08,t,g,00300,162,+\n",
      "a,35.75,0.915,u,g,aa,v,0.75,t,t,04,f,g,00000,1583,+\n",
      "b,59.50,2.75,u,g,w,v,1.75,t,t,05,t,g,00060,58,+\n",
      "b,21.00,3,y,p,d,v,1.085,t,t,08,t,g,00160,1,+\n",
      "b,21.92,0.54,y,p,x,v,0.04,t,t,01,t,g,00840,59,+\n",
      "a,65.17,14,u,g,ff,ff,0,t,t,11,t,g,00000,1400,+\n",
      "a,20.33,10,u,g,c,h,1,t,t,04,f,g,00050,1465,+\n",
      "b,32.25,0.165,y,p,c,h,3.25,t,t,01,t,g,00432,8000,+\n",
      "b,30.17,0.5,u,g,c,v,1.75,t,t,11,f,g,00032,540,+\n",
      "b,25.17,6,u,g,c,v,1,t,t,03,f,g,00000,0,+\n",
      "b,39.17,1.625,u,g,c,v,1.5,t,t,10,f,g,00186,4700,+\n",
      "b,39.08,6,u,g,m,v,1.29,t,t,05,t,g,00108,1097,+\n",
      "b,31.67,0.83,u,g,x,v,1.335,t,t,08,t,g,00303,3290,+\n",
      "b,41.00,0.04,u,g,e,v,0.04,f,t,01,f,s,00560,0,+\n",
      "b,48.50,4.25,u,g,m,v,0.125,t,f,0,t,g,00225,0,+\n",
      "b,32.67,9,y,p,w,h,5.25,t,f,0,t,g,00154,0,+\n",
      "a,28.08,15,y,p,e,z,0,t,f,0,f,g,00000,13212,+\n",
      "b,73.42,17.75,u,g,ff,ff,0,t,f,0,t,g,00000,0,+\n",
      "b,64.08,20,u,g,x,h,17.5,t,t,09,t,g,00000,1000,+\n",
      "b,51.58,15,u,g,c,v,8.5,t,t,09,f,g,00000,0,+\n",
      "b,26.67,1.75,y,p,c,v,1,t,t,05,t,g,00160,5777,+\n",
      "b,25.33,0.58,u,g,c,v,0.29,t,t,07,t,g,00096,5124,+\n",
      "b,30.17,6.5,u,g,cc,v,3.125,t,t,08,f,g,00330,1200,+\n",
      "b,27.00,0.75,u,g,c,h,4.25,t,t,03,t,g,00312,150,+\n",
      "b,34.17,5.25,u,g,w,v,0.085,f,f,0,t,g,00290,6,+\n",
      "b,38.67,0.21,u,g,k,v,0.085,t,f,0,t,g,00280,0,+\n",
      "b,25.75,0.75,u,g,c,bb,0.25,t,f,0,f,g,00349,23,+\n",
      "a,46.08,3,u,g,c,v,2.375,t,t,08,t,g,00396,4159,+\n",
      "a,21.50,6,u,g,aa,v,2.5,t,t,03,f,g,00080,918,+\n",
      "b,20.50,2.415,u,g,c,v,2,t,t,11,t,g,00200,3000,+\n",
      "a,29.50,0.46,u,g,k,v,0.54,t,t,04,f,g,00380,500,+\n",
      "b,29.83,1.25,y,p,k,v,0.25,f,f,0,f,g,00224,0,-\n",
      "b,20.08,0.25,u,g,q,v,0.125,f,f,0,f,g,00200,0,-\n",
      "b,23.42,0.585,u,g,c,h,0.085,t,f,0,f,g,00180,0,-\n",
      "a,29.58,1.75,y,p,k,v,1.25,f,f,0,t,g,00280,0,-\n",
      "b,16.17,0.04,u,g,c,v,0.04,f,f,0,f,g,00000,0,+\n",
      "b,32.33,3.5,u,g,k,v,0.5,f,f,0,t,g,00232,0,-\n",
      "b,47.83,4.165,u,g,x,bb,0.085,f,f,0,t,g,00520,0,-\n",
      "b,20.00,1.25,y,p,k,v,0.125,f,f,0,f,g,00140,4,-\n",
      "b,27.58,3.25,y,p,q,h,5.085,f,t,02,t,g,00369,1,-\n",
      "b,22.00,0.79,u,g,w,v,0.29,f,t,01,f,g,00420,283,-\n",
      "b,19.33,10.915,u,g,c,bb,0.585,f,t,02,t,g,00200,7,-\n",
      "a,38.33,4.415,u,g,c,v,0.125,f,f,0,f,g,00160,0,-\n",
      "b,29.42,1.25,u,g,c,h,0.25,f,t,02,t,g,00400,108,-\n",
      "b,22.67,0.75,u,g,i,v,1.585,f,t,01,t,g,00400,9,-\n",
      "b,32.25,14,y,p,ff,ff,0,f,t,02,f,g,00160,1,-\n",
      "b,29.58,4.75,u,g,m,v,2,f,t,01,t,g,00460,68,-\n",
      "b,18.42,10.415,y,p,aa,v,0.125,t,f,0,f,g,00120,375,-\n",
      "b,22.17,2.25,u,g,i,v,0.125,f,f,0,f,g,00160,10,-\n",
      "b,22.67,0.165,u,g,c,j,2.25,f,f,0,t,s,00000,0,+\n",
      "b,18.83,0,u,g,q,v,0.665,f,f,0,f,g,00160,1,-\n",
      "b,21.58,0.79,y,p,cc,v,0.665,f,f,0,f,g,00160,0,-\n",
      "b,23.75,12,u,g,c,v,2.085,f,f,0,f,s,00080,0,-\n",
      "b,36.08,2.54,u,g,ff,ff,0,f,f,0,f,g,00000,1000,-\n",
      "b,29.25,13,u,g,d,h,0.5,f,f,0,f,g,00228,0,-\n",
      "a,19.58,0.665,u,g,w,v,1.665,f,f,0,f,g,00220,5,-\n",
      "a,22.92,1.25,u,g,q,v,0.25,f,f,0,t,g,00120,809,-\n",
      "a,27.25,0.29,u,g,m,h,0.125,f,t,01,t,g,00272,108,-\n",
      "a,38.75,1.5,u,g,ff,ff,0,f,f,0,f,g,00076,0,-\n",
      "b,32.42,2.165,y,p,k,ff,0,f,f,0,f,g,00120,0,-\n",
      "a,23.75,0.71,u,g,w,v,0.25,f,t,01,t,g,00240,4,-\n",
      "b,18.17,2.46,u,g,c,n,0.96,f,t,02,t,g,00160,587,-\n",
      "b,40.92,0.5,y,p,m,v,0.5,f,f,0,t,g,00130,0,-\n",
      "b,19.50,9.585,u,g,aa,v,0.79,f,f,0,f,g,00080,350,-\n",
      "b,28.58,3.625,u,g,aa,v,0.25,f,f,0,t,g,00100,0,-\n",
      "b,35.58,0.75,u,g,k,v,1.5,f,f,0,t,g,00231,0,-\n",
      "b,34.17,2.75,u,g,i,bb,2.5,f,f,0,t,g,00232,200,-\n",
      "b,31.58,0.75,y,p,aa,v,3.5,f,f,0,t,g,00320,0,-\n",
      "a,52.50,7,u,g,aa,h,3,f,f,0,f,g,00000,0,-\n",
      "b,36.17,0.42,y,p,w,v,0.29,f,f,0,t,g,00309,2,-\n",
      "b,37.33,2.665,u,g,cc,v,0.165,f,f,0,t,g,00000,501,-\n",
      "a,20.83,8.5,u,g,c,v,0.165,f,f,0,f,g,00000,351,-\n",
      "b,24.08,9,u,g,aa,v,0.25,f,f,0,t,g,00000,0,-\n",
      "b,25.58,0.335,u,g,k,h,3.5,f,f,0,t,g,00340,0,-\n",
      "a,35.17,3.75,u,g,ff,ff,0,f,t,06,f,g,00000,200,-\n",
      "b,48.08,3.75,u,g,i,bb,1,f,f,0,f,g,00100,2,-\n",
      "a,15.83,7.625,u,g,q,v,0.125,f,t,01,t,g,00000,160,-\n",
      "a,22.50,0.415,u,g,i,v,0.335,f,f,0,t,s,00144,0,-\n",
      "b,21.50,11.5,u,g,i,v,0.5,t,f,0,t,g,00100,68,-\n",
      "a,23.58,0.83,u,g,q,v,0.415,f,t,01,t,g,00200,11,-\n",
      "a,21.08,5,y,p,ff,ff,0,f,f,0,f,g,00000,0,-\n",
      "b,25.67,3.25,u,g,c,h,2.29,f,t,01,t,g,00416,21,-\n",
      "a,38.92,1.665,u,g,aa,v,0.25,f,f,0,f,g,00000,390,-\n",
      "a,15.75,0.375,u,g,c,v,1,f,f,0,f,g,00120,18,-\n",
      "a,28.58,3.75,u,g,c,v,0.25,f,t,01,t,g,00040,154,-\n",
      "b,22.25,9,u,g,aa,v,0.085,f,f,0,f,g,00000,0,-\n",
      "b,29.83,3.5,u,g,c,v,0.165,f,f,0,f,g,00216,0,-\n",
      "a,23.50,1.5,u,g,w,v,0.875,f,f,0,t,g,00160,0,-\n",
      "b,32.08,4,y,p,cc,v,1.5,f,f,0,t,g,00120,0,-\n",
      "b,31.08,1.5,y,p,w,v,0.04,f,f,0,f,s,00160,0,-\n",
      "b,31.83,0.04,y,p,m,v,0.04,f,f,0,f,g,00000,0,-\n",
      "a,21.75,11.75,u,g,c,v,0.25,f,f,0,t,g,00180,0,-\n",
      "a,17.92,0.54,u,g,c,v,1.75,f,t,01,t,g,00080,5,-\n",
      "b,30.33,0.5,u,g,d,h,0.085,f,f,0,t,s,00252,0,-\n",
      "b,51.83,2.04,y,p,ff,ff,1.5,f,f,0,f,g,00120,1,-\n",
      "b,47.17,5.835,u,g,w,v,5.5,f,f,0,f,g,00465,150,-\n",
      "b,25.83,12.835,u,g,cc,v,0.5,f,f,0,f,g,00000,2,-\n",
      "a,50.25,0.835,u,g,aa,v,0.5,f,f,0,t,g,00240,117,-\n",
      "a,37.33,2.5,u,g,i,h,0.21,f,f,0,f,g,00260,246,-\n",
      "a,41.58,1.04,u,g,aa,v,0.665,f,f,0,f,g,00240,237,-\n",
      "a,30.58,10.665,u,g,q,h,0.085,f,t,12,t,g,00129,3,-\n",
      "b,19.42,7.25,u,g,m,v,0.04,f,t,01,f,g,00100,1,-\n",
      "a,17.92,10.21,u,g,ff,ff,0,f,f,0,f,g,00000,50,-\n",
      "a,20.08,1.25,u,g,c,v,0,f,f,0,f,g,00000,0,-\n",
      "b,19.50,0.29,u,g,k,v,0.29,f,f,0,f,g,00280,364,-\n",
      "b,27.83,1,y,p,d,h,3,f,f,0,f,g,00176,537,-\n",
      "b,17.08,3.29,u,g,i,v,0.335,f,f,0,t,g,00140,2,-\n",
      "b,36.42,0.75,y,p,d,v,0.585,f,f,0,f,g,00240,3,-\n",
      "b,40.58,3.29,u,g,m,v,3.5,f,f,0,t,s,00400,0,-\n",
      "b,21.08,10.085,y,p,e,h,1.25,f,f,0,f,g,00260,0,-\n",
      "a,22.67,0.75,u,g,c,v,2,f,t,02,t,g,00200,394,-\n",
      "a,25.25,13.5,y,p,ff,ff,2,f,t,01,t,g,00200,1,-\n",
      "b,17.92,0.205,u,g,aa,v,0.04,f,f,0,f,g,00280,750,-\n",
      "b,35.00,3.375,u,g,c,h,8.29,f,f,0,t,g,00000,0,-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove missing values\n",
    "cleaned_data = \"\"\n",
    "cleaned_p_count = 0\n",
    "cleaned_n_count = 0\n",
    "\n",
    "with open('data/crx.data', 'r') as f:\n",
    "    data = f.readlines()\n",
    "    for i, row in enumerate(data):\n",
    "        # Check for '?' value in each row (indicates missing)\n",
    "        if '?' not in row:\n",
    "            cleaned_data += row\n",
    "            if '+' in row:\n",
    "                cleaned_p_count += 1\n",
    "            elif '-' in row:\n",
    "                cleaned_n_count += 1\n",
    "\n",
    "    print(cleaned_data)\n",
    "\n",
    "with open('./data/crx_clean.data.txt', 'w') as f:\n",
    "    f.write(cleaned_data)\n",
    "\n",
    "with open('./data/crx_clean.names.txt', 'w') as f:\n",
    "    f.write(\"Class Distribution\\n\")\n",
    "    f.write(\"+ Classes: %d\\n\" %cleaned_p_count)\n",
    "    f.write(\"- Classes: %d\\n\" %cleaned_n_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_category(credit_data):\n",
    "    \"\"\"\n",
    "    Splits 'category' columns into one-hot columns\n",
    "    arg, return\n",
    "        credit_data: Dataframe\n",
    "    \"\"\"\n",
    "    cat_columns = []\n",
    "    for i, _ in enumerate(credit_data):\n",
    "        # dtype == 'object' after ensuring data has been cleaned\n",
    "        # i.e no 'float' dtypes as 'object' because of '?' values\n",
    "        if credit_data[i].dtype == 'object' and not i==15:\n",
    "            cat_columns.append(i)\n",
    "\n",
    "\n",
    "    # get_dummies() one-hot encodes data\n",
    "    credit_data = pd.get_dummies(credit_data, columns=cat_columns)\n",
    "    \n",
    "    return credit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 100\n",
    "\n",
    "def import_data(url):\n",
    "    \"\"\"\n",
    "    args\n",
    "        url: url string of CLEANED csv data\n",
    "    returns\n",
    "        credit_data: Dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    credit_data = pd.read_csv(url, sep=',', header=None)\n",
    "\n",
    "    # Bring class attribute to first column\n",
    "    cols = credit_data.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    credit_data = credit_data[cols]\n",
    "    print(\"Reordered Dataset: \\n\", credit_data.head())\n",
    "\n",
    "    credit_data = one_hot_encode_category(credit_data)\n",
    "    print(\"Dataset length: \", len(credit_data))\n",
    "    print(\"Dataset shape: \", credit_data.shape)\n",
    "    print(\"One-hot Dataset: \\n\", credit_data.head())\n",
    "    # print(credit_data.info())\n",
    "    return credit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordered Dataset: \n",
      "   15 0      1      2  3  4  5  6     7  8  9   10 11 12   13   14\n",
      "0  +  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  202    0\n",
      "1  +  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g   43  560\n",
      "2  +  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  280  824\n",
      "3  +  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  100    3\n",
      "4  +  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  120    0\n",
      "Dataset length:  653\n",
      "Dataset shape:  (653, 47)\n",
      "One-hot Dataset: \n",
      "   15      1      2     7  10   13   14  0_a  0_b  3_l  ...  6_z  8_f  8_t  \\\n",
      "0  +  30.83  0.000  1.25   1  202    0    0    1    0  ...    0    0    1   \n",
      "1  +  58.67  4.460  3.04   6   43  560    1    0    0  ...    0    0    1   \n",
      "2  +  24.50  0.500  1.50   0  280  824    1    0    0  ...    0    0    1   \n",
      "3  +  27.83  1.540  3.75   5  100    3    0    1    0  ...    0    0    1   \n",
      "4  +  20.17  5.625  1.71   0  120    0    0    1    0  ...    0    0    1   \n",
      "\n",
      "   9_f  9_t  11_f  11_t  12_g  12_p  12_s  \n",
      "0    0    1     1     0     1     0     0  \n",
      "1    0    1     1     0     1     0     0  \n",
      "2    1    0     1     0     1     0     0  \n",
      "3    0    1     0     1     1     0     0  \n",
      "4    1    0     1     0     0     0     1  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "#Read filtered data and feature setup and split to feed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Building Phase\n",
    "data = import_data(\"data/crx_clean.data.txt\")\n",
    "\n",
    "X = data.values[:, 1:]\n",
    "Y = data.values[:, 0]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = seed)\n",
    "\n",
    "#Float conversion\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "\n",
    "Y_train = np.where(Y_train=='+', 1, Y_train)\n",
    "Y_train = np.where(Y_train=='-', 0, Y_train)\n",
    "Y_train = Y_train.astype(np.float)\n",
    "\n",
    "Y_test = np.where(Y_test=='+', 1, Y_test)\n",
    "Y_test = np.where(Y_test=='-', 0, Y_test)\n",
    "Y_test = Y_test.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "targets = np.concatenate((Y_train, Y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(optimizerF, lossF):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(optimizerF, lossF):\n",
    "    print(f'**************************************{optimizerF}----{lossF}**************************************')\n",
    "    # Define per-fold score containers\n",
    "    f1_per_fold = []\n",
    "    loss_per_fold = []\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "    # Define the model architecture\n",
    "        model = get_model(optimizerF, lossF)\n",
    "#         print('-------------------------------------------------------------------------------------------------')\n",
    "#         print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "        history = model.fit(inputs[train], targets[train],batch_size=10,epochs=50, verbose=0)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "#         print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]} {model.metrics_names[1]} of {scores[1]}')\n",
    "        f1_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "    # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "\n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score per fold')\n",
    "    for i in range(0, len(f1_per_fold)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - F1 score: {f1_per_fold[i]}')\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('F1 scores for all folds:')\n",
    "    print(f'> F1: {np.mean(f1_per_fold)} (+- {np.std(f1_per_fold)})')\n",
    "    print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "    print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimzers = ['adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
    "lossFunctions = ['binary_crossentropy', 'mean_squared_error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************adam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.7380684614181519 - F1 score: 0.6606453657150269\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.3622402250766754 - F1 score: 0.8632726669311523\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.7761539816856384 - F1 score: 0.8000894784927368\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5851089954376221 - F1 score: 0.8237780332565308\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.800238847732544 - F1 score: 0.5476856231689453\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7390942335128784 (+- 0.11756149892541609)\n",
      "> Loss: 0.6523621022701264\n",
      "------------------------------------------------------------------------\n",
      "**************************************adam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.23129110038280487 - F1 score: 0.7455486059188843\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.26370474696159363 - F1 score: 0.7417405247688293\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.1879774034023285 - F1 score: 0.6078440546989441\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.25219008326530457 - F1 score: 0.5583401918411255\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.39730581641197205 - F1 score: 0.29965490102767944\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5906256556510925 (+- 0.1629967046743348)\n",
      "> Loss: 0.2664938300848007\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6830555200576782 - F1 score: 0.4463820457458496\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6703357696533203 - F1 score: 0.14315788447856903\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6964169144630432 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.66611647605896 - F1 score: 0.7282206416130066\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.7932962775230408 - F1 score: 0.639243483543396\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.39140081107616426 (+- 0.2802554799539834)\n",
      "> Loss: 0.7018441915512085\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.46189624071121216 - F1 score: 0.10937950760126114\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2414913773536682 - F1 score: 0.667516827583313\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.3790762424468994 - F1 score: 0.08571428060531616\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.24117523431777954 - F1 score: 0.5711308717727661\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.43410801887512207 - F1 score: 0.22222217917442322\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.33119273334741595 (+- 0.2416690158824495)\n",
      "> Loss: 0.35154942274093626\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4332358241081238 - F1 score: 0.7909174561500549\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 1.412292242050171 - F1 score: 0.6321145296096802\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5847283005714417 - F1 score: 0.7061071395874023\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.42447084188461304 - F1 score: 0.651727557182312\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.108007788658142 - F1 score: 0.8185964822769165\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7198926329612731 (+- 0.07393035667469545)\n",
      "> Loss: 0.7925469994544982\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.13712665438652039 - F1 score: 0.8490524291992188\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.36741018295288086 - F1 score: 0.5324548482894897\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.13688617944717407 - F1 score: 0.8538328409194946\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.286213219165802 - F1 score: 0.36315789818763733\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.1928797960281372 - F1 score: 0.5776190161705017\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6352234065532685 (+- 0.19047951014262862)\n",
      "> Loss: 0.2241032063961029\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 9.673251152038574 - F1 score: 0.6356251239776611\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 77.94671630859375 - F1 score: 0.12208416312932968\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 7.28421688079834 - F1 score: 0.4179008901119232\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 10.010571479797363 - F1 score: 0.49869856238365173\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 40.2471923828125 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.33486174792051315 (+- 0.23731974216089063)\n",
      "> Loss: 29.032389640808105\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4440195560455322 - F1 score: 0.16945531964302063\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.49008142948150635 - F1 score: 0.07072462886571884\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4306844472885132 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.40378424525260925 - F1 score: 0.019999997690320015\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.35833269357681274 - F1 score: 0.33333200216293335\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.11870238967239857 (+- 0.12229146780326484)\n",
      "> Loss: 0.42538047432899473\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----binary_crossentropy**************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5178340673446655 - F1 score: 0.6711379885673523\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.7362622618675232 - F1 score: 0.6934472322463989\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.47649747133255005 - F1 score: 0.7414284944534302\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.31766939163208 - F1 score: 0.5101239681243896\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5467340350151062 - F1 score: 0.5781061053276062\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6388487577438354 (+- 0.08343372401339955)\n",
      "> Loss: 0.718999445438385\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.18916372954845428 - F1 score: 0.6750877499580383\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6924872994422913 - F1 score: 0.36572542786598206\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.21811199188232422 - F1 score: 0.6006084680557251\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.37835371494293213 - F1 score: 0.12727269530296326\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.24912138283252716 - F1 score: 0.4724636971950531\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.44823160767555237 (+- 0.19240140548928208)\n",
      "> Loss: 0.34544762372970583\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.502528190612793 - F1 score: 0.7155154943466187\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.3954087495803833 - F1 score: 0.8494707345962524\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2751229703426361 - F1 score: 0.8995959162712097\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.46263226866722107 - F1 score: 0.629523754119873\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3679344952106476 - F1 score: 0.6682772636413574\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7524766325950623 (+- 0.10452273365211738)\n",
      "> Loss: 0.4007253348827362\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.2152237743139267 - F1 score: 0.7870112061500549\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.12970134615898132 - F1 score: 0.8524476885795593\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.14777173101902008 - F1 score: 0.8819084167480469\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4162944257259369 - F1 score: 0.20449873805046082\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.11889812350273132 - F1 score: 0.636175811290741\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6724083721637726 (+- 0.24887364579538562)\n",
      "> Loss: 0.20557788014411926\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.8168527483940125 - F1 score: 0.6990946531295776\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 2.8303375244140625 - F1 score: 0.7541124820709229\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5050445199012756 - F1 score: 0.8228319883346558\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.47760918736457825 - F1 score: 0.7061200141906738\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2906131446361542 - F1 score: 0.8657824397087097\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.769588315486908 (+- 0.06530739828689486)\n",
      "> Loss: 0.9840914249420166\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.18830813467502594 - F1 score: 0.7213479280471802\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.28059715032577515 - F1 score: 0.5345208644866943\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.36293983459472656 - F1 score: 0.3405616283416748\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.17579078674316406 - F1 score: 0.6219759583473206\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.14448587596416473 - F1 score: 0.6624571084976196\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.576172697544098 (+- 0.1325641255794177)\n",
      "> Loss: 0.2304243564605713\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5188520550727844 - F1 score: 0.752496600151062\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5572416186332703 - F1 score: 0.5620511770248413\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5120379328727722 - F1 score: 0.6124999523162842\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.7360362410545349 - F1 score: 0.4111892580986023\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6251171827316284 - F1 score: 0.47085028886795044\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5618174552917481 (+- 0.11822959491734882)\n",
      "> Loss: 0.5898570060729981\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.19566307961940765 - F1 score: 0.4732600152492523\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.21593685448169708 - F1 score: 0.47746577858924866\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.2501172423362732 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.24910251796245575 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2506139576435089 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.1901451587677002 (+- 0.23288310576063276)\n",
      "> Loss: 0.2322867304086685\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for opti in optimzers:\n",
    "    for los in lossFunctions:\n",
    "        evaluateModel(opti,los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************adam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4597422182559967 - F1 score: 0.8283277750015259\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5009877681732178 - F1 score: 0.6318618059158325\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5495796203613281 - F1 score: 0.7999998927116394\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.39907586574554443 - F1 score: 0.6696685552597046\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.9551959037780762 - F1 score: 0.5423809289932251\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6944477915763855 (+- 0.10650863144873225)\n",
      "> Loss: 0.5729162752628326\n",
      "------------------------------------------------------------------------\n",
      "**************************************adam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.31531670689582825 - F1 score: 0.4636150300502777\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.16522431373596191 - F1 score: 0.6166125535964966\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.1308925896883011 - F1 score: 0.8318487405776978\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.18097977340221405 - F1 score: 0.8197177648544312\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.14179806411266327 - F1 score: 0.786188006401062\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.703596419095993 (+- 0.14276656131273427)\n",
      "> Loss: 0.18684228956699372\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6675686240196228 - F1 score: 0.243647962808609\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6815239191055298 - F1 score: 0.13058821856975555\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6913196444511414 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6862170100212097 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.705660343170166 - F1 score: 0.08829130232334137\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.09250549674034118 (+- 0.09102376836856718)\n",
      "> Loss: 0.6864579081535339\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.26254263520240784 - F1 score: 0.7469622492790222\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2317635715007782 - F1 score: 0.4518098831176758\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.19601471722126007 - F1 score: 0.5501357316970825\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.23465923964977264 - F1 score: 0.7084539532661438\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2902384400367737 - F1 score: 0.668362557888031\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.625144875049591 (+- 0.10892110383381476)\n",
      "> Loss: 0.2430437207221985\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5458284616470337 - F1 score: 0.8477174639701843\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6173126697540283 - F1 score: 0.7864755392074585\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.49949371814727783 - F1 score: 0.8101286888122559\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.4193490445613861 - F1 score: 0.6628214716911316\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5095160007476807 - F1 score: 0.8319454193115234\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7878177165985107 (+- 0.06581622299119787)\n",
      "> Loss: 0.5182999789714813\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.2062384933233261 - F1 score: 0.6807147860527039\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1703343689441681 - F1 score: 0.8016303777694702\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.10956451296806335 - F1 score: 0.8618935346603394\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2897860109806061 - F1 score: 0.4774247109889984\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.16063493490219116 - F1 score: 0.6078550219535828\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.685903686285019 (+- 0.1370745021388927)\n",
      "> Loss: 0.18731166422367096\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 34.34354782104492 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 9.75411605834961 - F1 score: 0.3299590051174164\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 41.082298278808594 - F1 score: 0.2758859694004059\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 3.080232858657837 - F1 score: 0.48859643936157227\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 167.8253631591797 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.2188882827758789 (+- 0.1919125280142064)\n",
      "> Loss: 51.21711163520813\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4570019841194153 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.41765066981315613 - F1 score: 0.12333331257104874\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4249744713306427 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3239375054836273 - F1 score: 0.4761289954185486\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5292308330535889 - F1 score: 0.6285477876663208\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.24560201913118362 (+- 0.25899053598217797)\n",
      "> Loss: 0.4305590927600861\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----binary_crossentropy**************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.48175400495529175 - F1 score: 0.7713332772254944\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6529777646064758 - F1 score: 0.6641644835472107\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5858878493309021 - F1 score: 0.7445237040519714\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6554355621337891 - F1 score: 0.40840578079223633\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.5871337056159973 - F1 score: 0.5007023811340332\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6178259253501892 (+- 0.14095305441170192)\n",
      "> Loss: 0.5926377773284912\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.1793566644191742 - F1 score: 0.5798722505569458\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2912706434726715 - F1 score: 0.5666666030883789\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.22595159709453583 - F1 score: 0.7220528721809387\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3291627764701843 - F1 score: 0.2504594624042511\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.16571487486362457 - F1 score: 0.6093150973320007\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.545673257112503 (+- 0.1574303203021168)\n",
      "> Loss: 0.23829131126403807\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5913598537445068 - F1 score: 0.7446115016937256\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.444717675447464 - F1 score: 0.8659725189208984\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.38159218430519104 - F1 score: 0.6860648393630981\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6773533225059509 - F1 score: 0.6244117021560669\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3507690131664276 - F1 score: 0.8865200877189636\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7615161299705505 (+- 0.10130490065456678)\n",
      "> Loss: 0.48915840983390807\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.14236384630203247 - F1 score: 0.7509245872497559\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.13457460701465607 - F1 score: 0.7645020484924316\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.14396938681602478 - F1 score: 0.6216415166854858\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3436086177825928 - F1 score: 0.5386446118354797\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.1524055451154709 - F1 score: 0.6340872049331665\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.661959993839264 (+- 0.08489844198020384)\n",
      "> Loss: 0.1833844006061554\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3057507872581482 - F1 score: 0.910988986492157\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.39742398262023926 - F1 score: 0.7836008071899414\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.765017032623291 - F1 score: 0.7050994634628296\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6768614649772644 - F1 score: 0.5439773797988892\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.204872488975525 - F1 score: 0.8456481099128723\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7578629493713379 (+- 0.12676535591183283)\n",
      "> Loss: 0.6699851512908935\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.11226704716682434 - F1 score: 0.8697191476821899\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.1201038658618927 - F1 score: 0.870433509349823\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.18198397755622864 - F1 score: 0.5864158868789673\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.15650759637355804 - F1 score: 0.6226929426193237\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.17523865401744843 - F1 score: 0.7573120594024658\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.741314709186554 (+- 0.11956592177599776)\n",
      "> Loss: 0.14922022819519043\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6310371160507202 - F1 score: 0.4373794496059418\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6469810605049133 - F1 score: 0.5089380145072937\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5520680546760559 - F1 score: 0.5796296000480652\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.5482500195503235 - F1 score: 0.7209228873252869\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6748627424240112 - F1 score: 0.44902557134628296\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5391791045665741 (+- 0.10399269011456887)\n",
      "> Loss: 0.6106397986412049\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.24913501739501953 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.24910473823547363 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.24968783557415009 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.21943698823451996 - F1 score: 0.7799544930458069\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2551504075527191 - F1 score: 0.3719779849052429\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.23038649559020996 (+- 0.3102601958759188)\n",
      "> Loss: 0.24450299739837647\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_model(optimizerF, lossF):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model\n",
    "\n",
    "\n",
    "for opti in optimzers:\n",
    "    for los in lossFunctions:\n",
    "        evaluateModel(opti,los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************adam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.5775033831596375 - F1 score: 0.6335237622261047\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.49306952953338623 - F1 score: 0.814073920249939\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.3714781403541565 - F1 score: 0.8959332704544067\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.4474138021469116 - F1 score: 0.6190057992935181\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.7515568733215332 - F1 score: 0.8480879068374634\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7621249318122864 (+- 0.11403012661301182)\n",
      "> Loss: 0.728204345703125\n",
      "------------------------------------------------------------------------\n",
      "**************************************adam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.16242577135562897 - F1 score: 0.7743806838989258\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2790814936161041 - F1 score: 0.6591998338699341\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.22766181826591492 - F1 score: 0.4019289016723633\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3172638714313507 - F1 score: 0.30710020661354065\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.1759389489889145 - F1 score: 0.7290810942649841\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5743381440639496 (+- 0.18563672419651478)\n",
      "> Loss: 0.23247438073158264\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6966268420219421 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.6858587265014648 - F1 score: 0.20984847843647003\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6817946434020996 - F1 score: 0.1101343035697937\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6823946237564087 - F1 score: 0.053333330899477005\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.649756133556366 - F1 score: 0.4145786762237549\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.15757895782589912 (+- 0.14610286235505224)\n",
      "> Loss: 0.6792861938476562\n",
      "------------------------------------------------------------------------\n",
      "**************************************SGD----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3466704785823822 - F1 score: 0.09411764144897461\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2602498531341553 - F1 score: 0.6209187507629395\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.4379304051399231 - F1 score: 0.20539212226867676\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.24954472482204437 - F1 score: 0.07857142388820648\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.4476469159126282 - F1 score: 0.14773108065128326\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.2293462038040161 (+- 0.20079028074914773)\n",
      "> Loss: 0.3484084755182266\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.3898279666900635 - F1 score: 0.8410011529922485\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 3.526395559310913 - F1 score: 0.7533362507820129\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5861439108848572 - F1 score: 0.6394184231758118\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.38643398880958557 - F1 score: 0.8692063093185425\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.6702241897583008 - F1 score: 0.8725839853286743\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.795109224319458 (+- 0.08897571870461707)\n",
      "> Loss: 1.111805123090744\n",
      "------------------------------------------------------------------------\n",
      "**************************************RMSprop----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.14244286715984344 - F1 score: 0.6146936416625977\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.09637634456157684 - F1 score: 0.822961688041687\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.18607929348945618 - F1 score: 0.8037878274917603\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.1745031774044037 - F1 score: 0.7237054109573364\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.19047664105892181 - F1 score: 0.8019354939460754\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7534168124198913 (+- 0.07727182775564083)\n",
      "> Loss: 0.1579756647348404\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 179.05984497070312 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 23.642566680908203 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 2.411015272140503 - F1 score: 0.605003297328949\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 11.705257415771484 - F1 score: 0.500244140625\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 2.855461597442627 - F1 score: 0.5400694012641907\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.32906336784362794 (+- 0.2707525246077875)\n",
      "> Loss: 43.93482918739319\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adadelta----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.42867982387542725 - F1 score: 0.17982454597949982\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.641308605670929 - F1 score: 0.396845281124115\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5545142889022827 - F1 score: 0.49641671776771545\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.3843424916267395 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.3995873034000397 - F1 score: 0.5019142031669617\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.3150001496076584 (+- 0.19598214114708157)\n",
      "> Loss: 0.4816865026950836\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----binary_crossentropy**************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 1.2316051721572876 - F1 score: 0.4767506718635559\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5735089182853699 - F1 score: 0.7539047002792358\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6088120937347412 - F1 score: 0.6503904461860657\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6559863090515137 - F1 score: 0.4659718871116638\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 1.1095212697982788 - F1 score: 0.5223754644393921\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5738786339759827 (+- 0.11134965242115694)\n",
      "> Loss: 0.8358867526054382\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adagrad----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.31999900937080383 - F1 score: 0.7278913259506226\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5405983328819275 - F1 score: 0.6538985371589661\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.19750607013702393 - F1 score: 0.6770898699760437\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.388160765171051 - F1 score: 0.1836024522781372\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.2205697000026703 - F1 score: 0.7146096229553223\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5914183616638183 (+- 0.20560065780745215)\n",
      "> Loss: 0.33336677551269533\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.4498341977596283 - F1 score: 0.689408540725708\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.39697057008743286 - F1 score: 0.8671550750732422\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.44439756870269775 - F1 score: 0.7420521974563599\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.359260618686676 - F1 score: 0.8755452036857605\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.40847069025039673 - F1 score: 0.8031063079833984\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7954534649848938 (+- 0.07170996421438197)\n",
      "> Loss: 0.41178672909736636\n",
      "------------------------------------------------------------------------\n",
      "**************************************Adamax----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.16781945526599884 - F1 score: 0.8132010698318481\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.17514388263225555 - F1 score: 0.5090417861938477\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.41479048132896423 - F1 score: 0.480186402797699\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.2328924834728241 - F1 score: 0.7195237278938293\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.16395430266857147 - F1 score: 0.8428505063056946\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.6729606986045837 (+- 0.151478951111388)\n",
      "> Loss: 0.23092012107372284\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.38666144013404846 - F1 score: 0.6520075798034668\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.43022364377975464 - F1 score: 0.7662744522094727\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.7614173889160156 - F1 score: 0.7763147950172424\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.7481700778007507 - F1 score: 0.896731972694397\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.4558708071708679 - F1 score: 0.8197347521781921\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7822127103805542 (+- 0.07970540078229033)\n",
      "> Loss: 0.5564686715602875\n",
      "------------------------------------------------------------------------\n",
      "**************************************Nadam----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.2036454677581787 - F1 score: 0.8031694293022156\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.212936669588089 - F1 score: 0.5538890957832336\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.21344025433063507 - F1 score: 0.7473496794700623\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.1533721387386322 - F1 score: 0.829077422618866\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.219524547457695 - F1 score: 0.7792433500289917\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7425457954406738 (+- 0.09810615232986349)\n",
      "> Loss: 0.200583815574646\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6181594133377075 - F1 score: 0.5188033580780029\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.5206243395805359 - F1 score: 0.6037486791610718\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.5217300653457642 - F1 score: 0.691153347492218\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.6065546870231628 - F1 score: 0.4288390278816223\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.7209113240242004 - F1 score: 0.4682576060295105\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.5421604037284851 (+- 0.09470049870536401)\n",
      "> Loss: 0.5975959658622741\n",
      "------------------------------------------------------------------------\n",
      "**************************************Ftrl----mean_squared_error**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.24919159710407257 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 0.2493242770433426 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.24340985715389252 - F1 score: 0.46189507842063904\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 0.24961841106414795 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.25031334161758423 - F1 score: 0.0\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.0923790156841278 (+- 0.1847580313682556)\n",
      "> Loss: 0.24837149679660797\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_model(optimizerF, lossF):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model\n",
    "\n",
    "\n",
    "for opti in optimzers:\n",
    "    for los in lossFunctions:\n",
    "        evaluateModel(opti,los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************RMSprop----binary_crossentropy**************************************\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 0.6171974539756775 - F1 score: 0.8058371543884277\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 2.969980478286743 - F1 score: 0.6206064224243164\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 0.6620937585830688 - F1 score: 0.8579448461532593\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 1.1280773878097534 - F1 score: 0.6322553753852844\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 0.9970367550849915 - F1 score: 0.7748794555664062\n",
      "------------------------------------------------------------------------\n",
      "F1 scores for all folds:\n",
      "> F1: 0.7383046507835388 (+- 0.09519606452742523)\n",
      "> Loss: 1.274877166748047\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2,l1\n",
    "\n",
    "def get_model(optimizerF, lossF):\n",
    "    lamda=0.1\n",
    "    model = Sequential()\n",
    "    model.add(Dense(46, input_dim=46, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu', kernel_regularizer=l2(lamda), bias_regularizer=l2(lamda)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss=lossF,optimizer=optimizerF,metrics=[f1_m])\n",
    "    return model\n",
    "evaluateModel('RMSprop','binary_crossentropy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
